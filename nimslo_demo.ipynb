{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nimslo Image Alignment Demo\n",
    "\n",
    "This notebook demonstrates the complete pipeline for aligning Nimslo 4-lens camera images and generating boomerang GIFs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Basic imports successful\n",
      "✓ Core modules loaded (segmentation deferred)\n",
      "✓ All modules loaded successfully!\n",
      "✓ Brightness normalization available (normalize_brightness, brightness_strength)\n",
      "\n",
      "⚠ Note: Segmentation will be loaded on-demand to avoid kernel crashes\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# CRITICAL: Set these BEFORE importing anything that uses OpenMP (rembg/onnxruntime)\n",
    "# This prevents kernel crashes from OMP conflicts and fixes deprecated warnings\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "os.environ['OMP_NUM_THREADS'] = '1'  # Limit threads to avoid conflicts\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['OMP_MAX_ACTIVE_LEVELS'] = '1'  # Use max_active_levels instead of deprecated nested\n",
    "\n",
    "# Try to configure OpenMP programmatically\n",
    "try:\n",
    "    import ctypes\n",
    "    try:\n",
    "        # Set max_active_levels directly if possible\n",
    "        libomp = ctypes.CDLL(None)\n",
    "        if hasattr(libomp, 'omp_set_max_active_levels'):\n",
    "            libomp.omp_set_max_active_levels(1)\n",
    "            print(\"✓ OpenMP configured to use max_active_levels\")\n",
    "    except:\n",
    "        pass\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', message='.*omp_set_nested.*')\n",
    "\n",
    "# Add code directory to path\n",
    "code_dir = Path().absolute()\n",
    "if str(code_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(code_dir))\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "print(\"✓ Basic imports successful\")\n",
    "\n",
    "# Import and reload modules to pick up any changes (useful during development)\n",
    "try:\n",
    "    import nimslo_core.gif_generator\n",
    "    importlib.reload(nimslo_core.gif_generator)\n",
    "except Exception as e:\n",
    "    print(f\"Note: Could not reload module (this is OK on first run): {e}\")\n",
    "\n",
    "from nimslo_core import (\n",
    "    preprocess_image,\n",
    "    align_images,\n",
    "    extract_features,\n",
    "    match_features\n",
    ")\n",
    "\n",
    "# Import create_boomerang_gif AFTER reload to get latest version\n",
    "from nimslo_core.gif_generator import create_boomerang_gif\n",
    "\n",
    "# DON'T import segmentation here - it will crash the kernel\n",
    "# We'll import it only when needed, or use depth-based segmentation\n",
    "print(\"✓ Core modules loaded (segmentation deferred)\")\n",
    "\n",
    "# Verify the function has the expected parameters\n",
    "import inspect\n",
    "sig = inspect.signature(create_boomerang_gif)\n",
    "has_normalize = 'normalize_brightness' in sig.parameters\n",
    "has_strength = 'brightness_strength' in sig.parameters\n",
    "\n",
    "print(\"✓ All modules loaded successfully!\")\n",
    "if has_normalize and has_strength:\n",
    "    print(\"✓ Brightness normalization available (normalize_brightness, brightness_strength)\")\n",
    "else:\n",
    "    print(\"⚠ Warning: Brightness normalization parameters not found - restart kernel if needed\")\n",
    "print(\"\\n⚠ Note: Segmentation will be loaded on-demand to avoid kernel crashes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "RAW_DIR = Path(\"../nimslo_raw\")\n",
    "OUTPUT_DIR = Path(\"../outputs\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "BATCH_NAME = \"12\"  # Change this to process different batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Available Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19 batches.\n"
     ]
    }
   ],
   "source": [
    "# Find all batch directories\n",
    "batch_dirs = sorted([\n",
    "    d for d in RAW_DIR.iterdir()\n",
    "    if d.is_dir() and (d.name.isdigit() or d.name.replace(\"-\", \"\").isdigit())\n",
    "])\n",
    "\n",
    "print(f\"Found {len(batch_dirs)} batches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4 images from 12\n",
      "Original image dimensions: (2137, 1535, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load images from selected batch\n",
    "batch_path = RAW_DIR / BATCH_NAME\n",
    "image_files = sorted(batch_path.glob('*.jpg')) + sorted(batch_path.glob('*.JPG'))\n",
    "image_files = image_files[:4]  # Take first 4 images\n",
    "\n",
    "# Store original images (before any processing)\n",
    "original_images = []\n",
    "for f in image_files:\n",
    "    img = cv2.imread(str(f))\n",
    "    if img is not None:\n",
    "        original_images.append(img)\n",
    "\n",
    "# Normalize sizes of originals (just cropping, no denoising/contrast adjustment)\n",
    "from nimslo_core.preprocessing import normalize_sizes\n",
    "original_images = normalize_sizes(original_images)\n",
    "\n",
    "print(f\"Loaded {len(original_images)} images from {BATCH_NAME}\")\n",
    "print(f\"Original image dimensions: {original_images[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approach 1: Skipping denoising (SIFT is robust to noise)...\n",
      "  ✓ Preprocessed 4 images (exposure balanced, no denoising)\n",
      "\n",
      "Preprocessed image dimensions: (2137, 1535, 3)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PREPROCESSING CONFIGURATION\n",
    "# ============================================================================\n",
    "# Choose preprocessing approach to avoid kernel crashes:\n",
    "#\n",
    "# APPROACH 1: Skip denoising (safest, SIFT is robust to noise)\n",
    "#   - Set USE_DENOISING = False\n",
    "#   - Fastest, zero crash risk\n",
    "#\n",
    "# APPROACH 2: Downscale → denoise → use for alignment (safer with denoising)\n",
    "#   - Set USE_DENOISING = True and SAFE_PROCESSING_SIZE = 1024\n",
    "#   - Denoise runs on smaller images, then we use them for alignment\n",
    "#   - Transformations are applied to full-res originals for final GIF\n",
    "# ============================================================================\n",
    "\n",
    "USE_DENOISING = False  # Set to True for Approach 2\n",
    "SAFE_PROCESSING_SIZE = 1024  # Max dimension for safe denoising (only used if USE_DENOISING=True)\n",
    "\n",
    "from nimslo_core.preprocessing import resize_for_processing\n",
    "\n",
    "if USE_DENOISING:\n",
    "    # APPROACH 2: Downscale for safe denoising\n",
    "    print(f\"Approach 2: Downscaling to {SAFE_PROCESSING_SIZE}px for safe denoising...\")\n",
    "    \n",
    "    # Downscale images for processing\n",
    "    small_images = []\n",
    "    scales = []\n",
    "    for img in original_images:\n",
    "        small, scale = resize_for_processing(img, max_dimension=SAFE_PROCESSING_SIZE)\n",
    "        small_images.append(small)\n",
    "        scales.append(scale)\n",
    "    \n",
    "    print(f\"  Downscaled from {original_images[0].shape[:2]} to {small_images[0].shape[:2]} (scale: {scales[0]:.2f})\")\n",
    "    \n",
    "    # Denoise the smaller images (safe)\n",
    "    preprocessed = [preprocess_image(img, denoise=True) for img in small_images]\n",
    "    print(f\"  ✓ Denoised {len(preprocessed)} images at safe resolution\")\n",
    "    \n",
    "else:\n",
    "    # APPROACH 1: Skip denoising entirely (safest)\n",
    "    print(\"Approach 1: Skipping denoising (SIFT is robust to noise)...\")\n",
    "    \n",
    "    # Just balance exposure, no denoising\n",
    "    preprocessed = [preprocess_image(img, denoise=False, balance=True) for img in original_images]\n",
    "    print(f\"  ✓ Preprocessed {len(preprocessed)} images (exposure balanced, no denoising)\")\n",
    "\n",
    "print(f\"\\nPreprocessed image dimensions: {preprocessed[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation configuration:\n",
      "  USE_SEGMENTATION = False\n",
      "  USE_CENTER_MASK = True\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SEGMENTATION CONFIGURATION\n",
    "# ============================================================================\n",
    "# The depth model (Intel DPT) can also crash on large images.\n",
    "# Options:\n",
    "#\n",
    "# APPROACH A: Skip segmentation, use full image (safest)\n",
    "#   - Set USE_SEGMENTATION = False\n",
    "#   - SIFT will find features across entire image\n",
    "#   - Works well when subject is prominent\n",
    "#\n",
    "# APPROACH B: Use center mask (no model needed)\n",
    "#   - Set USE_SEGMENTATION = False and USE_CENTER_MASK = True\n",
    "#   - Assumes subject is roughly centered\n",
    "#\n",
    "# APPROACH C: Run depth segmentation (may crash on large images)\n",
    "#   - Set USE_SEGMENTATION = True\n",
    "#   - Only try this if Approach 1/2 preprocessing downscaled the images\n",
    "# ============================================================================\n",
    "\n",
    "USE_SEGMENTATION = False  # Set to True to run depth model (risky on large images)\n",
    "USE_CENTER_MASK = True    # If not using segmentation, use center mask vs full image\n",
    "\n",
    "print(\"Segmentation configuration:\")\n",
    "print(f\"  USE_SEGMENTATION = {USE_SEGMENTATION}\")\n",
    "print(f\"  USE_CENTER_MASK = {USE_CENTER_MASK}\")\n",
    "\n",
    "# Note: To use full segmentation safely, use the CLI instead:\n",
    "# python nimslo_cli.py ../nimslo_raw/01/ -o test.gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 1: Error - name 'segment_subject' is not defined\n",
      "Frame 1: Using fallback mask (center region)\n",
      "Frame 2: Error - name 'segment_subject' is not defined\n",
      "Frame 2: Using fallback mask (center region)\n",
      "Frame 3: Error - name 'segment_subject' is not defined\n",
      "Frame 3: Using fallback mask (center region)\n",
      "Frame 4: Error - name 'segment_subject' is not defined\n",
      "Frame 4: Using fallback mask (center region)\n"
     ]
    }
   ],
   "source": [
    "# Generate masks based on configuration\n",
    "masks = []\n",
    "\n",
    "def create_center_mask(img, margin=0.15):\n",
    "    \"\"\"Create a mask covering the center region of the image.\"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    y1, y2 = int(h * margin), int(h * (1 - margin))\n",
    "    x1, x2 = int(w * margin), int(w * (1 - margin))\n",
    "    mask[y1:y2, x1:x2] = 255\n",
    "    return mask\n",
    "\n",
    "def create_full_mask(img):\n",
    "    \"\"\"Create a mask covering the entire image (no masking).\"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    return np.ones((h, w), dtype=np.uint8) * 255\n",
    "\n",
    "if USE_SEGMENTATION:\n",
    "    # APPROACH C: Run depth segmentation (may crash on large images)\n",
    "    print(\"Running depth-based segmentation (may be slow/crash on large images)...\")\n",
    "    from nimslo_core.segmentation import segment_subject\n",
    "    \n",
    "    for i, img in enumerate(preprocessed):\n",
    "        try:\n",
    "            mask, conf = segment_subject(img, method=\"depth\", return_confidence=True)\n",
    "            masks.append(mask)\n",
    "            print(f\"  Frame {i+1}: confidence={conf:.2f}, method=depth\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Frame {i+1}: Error - {e}, using center mask fallback\")\n",
    "            masks.append(create_center_mask(img))\n",
    "else:\n",
    "    # APPROACH A or B: Skip segmentation model\n",
    "    if USE_CENTER_MASK:\n",
    "        print(\"Using center masks (no segmentation model)...\")\n",
    "        for i, img in enumerate(preprocessed):\n",
    "            masks.append(create_center_mask(img))\n",
    "            print(f\"  Frame {i+1}: center mask (15% margin)\")\n",
    "    else:\n",
    "        print(\"Using full image masks (no masking)...\")\n",
    "        for i, img in enumerate(preprocessed):\n",
    "            masks.append(create_full_mask(img))\n",
    "            print(f\"  Frame {i+1}: full image mask\")\n",
    "\n",
    "print(f\"\\n✓ Generated {len(masks)} masks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Visualize segmentation masks overlaid on images\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mnimslo_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msegmentation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m visualize_mask\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m fig, axes = \u001b[43mplt\u001b[49m.subplots(\u001b[32m2\u001b[39m, \u001b[32m4\u001b[39m, figsize=(\u001b[32m16\u001b[39m, \u001b[32m8\u001b[39m))\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Top row: original preprocessed images\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, (ax, img) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(axes[\u001b[32m0\u001b[39m], preprocessed)):\n",
      "\u001b[31mNameError\u001b[39m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Visualize masks overlaid on images\n",
    "# Define visualize_mask inline to avoid importing segmentation module (which may crash)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_mask_simple(img, mask, alpha=0.4, color=(0, 255, 0)):\n",
    "    \"\"\"Overlay a mask on an image with transparency.\"\"\"\n",
    "    overlay = img.copy()\n",
    "    mask_bool = mask > 127\n",
    "    overlay[mask_bool] = (\n",
    "        (1 - alpha) * overlay[mask_bool] + \n",
    "        alpha * np.array(color)\n",
    "    ).astype(np.uint8)\n",
    "    return overlay\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "# Top row: original preprocessed images\n",
    "for i, (ax, img) in enumerate(zip(axes[0], preprocessed)):\n",
    "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    ax.set_title(f\"Frame {i+1} (Preprocessed)\")\n",
    "    ax.axis('off')\n",
    "\n",
    "# Bottom row: images with masks overlaid\n",
    "mask_type = \"center\" if USE_CENTER_MASK else (\"depth\" if USE_SEGMENTATION else \"full\")\n",
    "for i, (ax, img, mask) in enumerate(zip(axes[1], preprocessed, masks)):\n",
    "    overlay = visualize_mask_simple(img, mask, alpha=0.3, color=(0, 255, 0))\n",
    "    ax.imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB))\n",
    "    ax.set_title(f\"Frame {i+1} ({mask_type} mask)\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(\"Subject Segmentation Results\", fontsize=14, y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction and Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from first two frames\n",
    "kp1, des1 = extract_features(preprocessed[0], mask=masks[0], n_features=1000)\n",
    "kp2, des2 = extract_features(preprocessed[1], mask=masks[1], n_features=1000)\n",
    "\n",
    "print(f\"Frame 1: {len(kp1)} keypoints, {des1.shape[0] if des1 is not None else 0} descriptors\")\n",
    "print(f\"Frame 2: {len(kp2)} keypoints, {des2.shape[0] if des2 is not None else 0} descriptors\")\n",
    "\n",
    "# Match features (match_features already applies ratio test)\n",
    "matches = match_features(des1, des2)\n",
    "\n",
    "print(f\"\\nMatches after ratio test: {len(matches)}\")\n",
    "if len(matches) > 0:\n",
    "    distances = [m.distance for m in matches]\n",
    "    print(f\"  Distance range: {min(distances):.2f} - {max(distances):.2f}\")\n",
    "    print(f\"  Mean distance: {np.mean(distances):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize keypoints detected on each image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Frame 1: Show keypoints\n",
    "img1_kp = cv2.drawKeypoints(\n",
    "    preprocessed[0], kp1, None,\n",
    "    flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n",
    ")\n",
    "axes[0].imshow(cv2.cvtColor(img1_kp, cv2.COLOR_BGR2RGB))\n",
    "axes[0].set_title(f\"Frame 1: {len(kp1)} keypoints\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Frame 2: Show keypoints\n",
    "img2_kp = cv2.drawKeypoints(\n",
    "    preprocessed[1], kp2, None,\n",
    "    flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n",
    ")\n",
    "axes[1].imshow(cv2.cvtColor(img2_kp, cv2.COLOR_BGR2RGB))\n",
    "axes[1].set_title(f\"Frame 2: {len(kp2)} keypoints\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.suptitle(\"Feature Keypoints Detection\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize matches (use matches from match_features, not filtered)\n",
    "if len(matches) > 0:\n",
    "    # Sort by distance to show best matches first\n",
    "    sorted_matches = sorted(matches, key=lambda x: x.distance)\n",
    "    \n",
    "    img_matches = cv2.drawMatches(\n",
    "        preprocessed[0], kp1,\n",
    "        preprocessed[1], kp2,\n",
    "        sorted_matches[:50],  # Show best 50 matches\n",
    "        None,\n",
    "        flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.imshow(cv2.cvtColor(img_matches, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f\"Feature Matches Between Frame 1 and Frame 2 (showing best {min(50, len(matches))} of {len(matches)})\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No matches found! This could indicate:\")\n",
    "    print(\"  - Masks are too restrictive (not enough overlap)\")\n",
    "    print(\"  - Images are too different\")\n",
    "    print(\"  - Feature extraction failed\")\n",
    "    \n",
    "    # Try without masks to see if that helps\n",
    "    print(\"\\nTrying without masks...\")\n",
    "    kp1_no_mask, des1_no_mask = extract_features(preprocessed[0], mask=None, n_features=1000)\n",
    "    kp2_no_mask, des2_no_mask = extract_features(preprocessed[1], mask=None, n_features=1000)\n",
    "    matches_no_mask = match_features(des1_no_mask, des2_no_mask)\n",
    "    print(f\"Without masks: {len(matches_no_mask)} matches\")\n",
    "    \n",
    "    if len(matches_no_mask) > 0:\n",
    "        sorted_matches = sorted(matches_no_mask, key=lambda x: x.distance)\n",
    "        img_matches = cv2.drawMatches(\n",
    "            preprocessed[0], kp1_no_mask,\n",
    "            preprocessed[1], kp2_no_mask,\n",
    "            sorted_matches[:50],\n",
    "            None,\n",
    "            flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    "        )\n",
    "        plt.figure(figsize=(16, 8))\n",
    "        plt.imshow(cv2.cvtColor(img_matches, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f\"Matches WITHOUT masks (showing best {min(50, len(matches_no_mask))} of {len(matches_no_mask)})\")\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align preprocessed images (for good feature matching)\n",
    "# This gives us the transformation matrices\n",
    "aligned_preprocessed, alignment_results = align_images(\n",
    "    preprocessed, masks,\n",
    "    n_features=1000\n",
    ")\n",
    "\n",
    "print(\"Alignment results:\")\n",
    "for i, result in enumerate(alignment_results):\n",
    "    if i > 0:  # Skip reference frame\n",
    "        print(f\"Frame {i+1}: {result.total_matches} matches, {result.inliers} inliers, IoU: {result.iou:.2f}\")\n",
    "\n",
    "# Apply the same transformations to original images (max quality)\n",
    "print(\"\\nApplying transformations to original images...\")\n",
    "aligned_originals = []\n",
    "ref_h, ref_w = original_images[0].shape[:2]\n",
    "\n",
    "# Check if we need to scale the homography (Approach 2: preprocessed at different resolution)\n",
    "preproc_h, preproc_w = preprocessed[0].shape[:2]\n",
    "need_scale = (preproc_h != ref_h) or (preproc_w != ref_w)\n",
    "\n",
    "if need_scale:\n",
    "    # Scale factors from preprocessed to original\n",
    "    scale_x = ref_w / preproc_w\n",
    "    scale_y = ref_h / preproc_h\n",
    "    print(f\"  Scaling transforms from {preproc_w}x{preproc_h} to {ref_w}x{ref_h}\")\n",
    "    \n",
    "    # Scaling matrices: S scales up, S_inv scales down\n",
    "    S = np.array([[scale_x, 0, 0], [0, scale_y, 0], [0, 0, 1]], dtype=np.float64)\n",
    "    S_inv = np.array([[1/scale_x, 0, 0], [0, 1/scale_y, 0], [0, 0, 1]], dtype=np.float64)\n",
    "\n",
    "for i, (orig_img, result) in enumerate(zip(original_images, alignment_results)):\n",
    "    if i == 0:\n",
    "        # Reference frame stays as-is\n",
    "        aligned_originals.append(orig_img.copy())\n",
    "    else:\n",
    "        # Apply transformation from alignment result\n",
    "        # transform is stored as 3x3 (affine padded or homography)\n",
    "        transform = result.transform\n",
    "        \n",
    "        if need_scale:\n",
    "            # Scale homography: H_full = S * H_small * S_inv\n",
    "            transform = S @ transform @ S_inv\n",
    "        \n",
    "        aligned_originals.append(cv2.warpPerspective(orig_img, transform, (ref_w, ref_h)))\n",
    "\n",
    "print(\"✓ Original images aligned (max quality, no denoising/contrast adjustment)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display aligned original images (max quality)\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for i, (ax, img) in enumerate(zip(axes, aligned_originals)):\n",
    "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    ax.set_title(f\"Aligned Original Frame {i+1}\")\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Boomerang GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GIF from aligned original images (max quality, no denoising/contrast adjustment)\n",
    "# Optionally resize for web-friendly size (or use full resolution for max quality)\n",
    "from nimslo_core.gif_generator import resize_for_web\n",
    "\n",
    "# For max quality, use full resolution. For web-friendly, uncomment the resize line:\n",
    "aligned_originals = resize_for_web(aligned_originals, max_dimension=600)\n",
    "\n",
    "# Create boomerang GIF from original images\n",
    "# crop_valid_region=True removes black bars from stereoscopic alignment\n",
    "# normalize_brightness=True prevents flashing from exposure differences\n",
    "# brightness_strength controls how much correction (0.0-1.0, default 0.5 = moderate)\n",
    "output_path = OUTPUT_DIR / f\"test_{BATCH_NAME}.gif\"\n",
    "gif_path = create_boomerang_gif(\n",
    "    aligned_originals, \n",
    "    output_path,\n",
    "    crop_valid_region=True,  # Remove black borders from warping\n",
    "    normalize_brightness=True,  # Equalize brightness to prevent flashing\n",
    "    brightness_strength=0.5  # Moderate correction (0.0 = none, 1.0 = full)\n",
    ")\n",
    "\n",
    "print(f\"✓ GIF saved to: {gif_path}\")\n",
    "print(f\"  File size: {gif_path.stat().st_size / 1024:.1f} KB\")\n",
    "print(f\"  Using original images (max quality, no denoising/contrast adjustment)\")\n",
    "print(f\"  Cropped to remove black bars from alignment\")\n",
    "print(f\"  Brightness normalized across frames to prevent flashing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the GIF (if in Jupyter)\n",
    "from IPython.display import Image, display\n",
    "display(Image(str(gif_path)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
